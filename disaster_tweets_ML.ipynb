{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9167ac13",
   "metadata": {},
   "source": [
    "# Eye of the Emergency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd82f9",
   "metadata": {},
   "source": [
    "## Libraries and dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9486461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\utile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\utile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\utile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\utile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data import & Exploring\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1db272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_tweets.csv')\n",
    "test_df = pd.read_csv('test_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95700639",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1460ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = pd.DataFrame(pd.value_counts(train_df.text))\n",
    "unique_count = unique_count.reset_index()\n",
    "unique_count.columns = ['text', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58be70b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#unique_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee09db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_unique = pd.DataFrame(train_df.text.unique(), columns = ['text'])\n",
    "#train_df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17533f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_unique = train_df_unique.merge(unique_count)\n",
    "#train_df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9acccc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_count = train_df.merge(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e17a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b09cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = pd.DataFrame([len(txt) for txt in train_df.text], columns = ['len_text'])\n",
    "#lg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d093c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_len = pd.concat([train_df_count, lg], axis = 1)\n",
    "#train_df_len.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab696116",
   "metadata": {},
   "source": [
    "### Location Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e201b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_location = pd.DataFrame([int(type(ct)==str) for ct in train_df_len.location], columns=['location_enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ea22e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>count</th>\n",
       "      <th>len_text</th>\n",
       "      <th>location_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  count  len_text  location_enc  \n",
       "0       1      1        69             0  \n",
       "1       1      1        38             0  \n",
       "2       1      1       133             0  \n",
       "3       1      1        65             0  \n",
       "4       1      1        88             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_location = pd.concat([train_df_len, encode_location], axis=1)\n",
    "train_df_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda11580",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56dd6338",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df_location.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1656712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = train_df_location.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e97ba05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "\n",
    "def text_processing(text):\n",
    "     #Charger les stop-words en anglais\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Initialiser le lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Appliquer la tokenisation à tous les textes\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Supprimer les ponctuations\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "\n",
    "    # Supprimer les stop-words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "\n",
    "    # Appliquer la lemmatisation à tous les tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    #tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "# Charger l'ensemble de données\n",
    "#df = pd.read_csv('votre_fichier.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ff1f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction Cleanup à tous les textes dans la colonne \"text\"\n",
    "df2['text_process'] = df['text'].apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9dfc969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>count</th>\n",
       "      <th>len_text</th>\n",
       "      <th>location_enc</th>\n",
       "      <th>text_process</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>[resident, asked, 'shelter, place, notified, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>[13,000, people, receive, wildfire, evacuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  count  len_text  location_enc  \\\n",
       "0       1      1        69             0   \n",
       "1       1      1        38             0   \n",
       "2       1      1       133             0   \n",
       "3       1      1        65             0   \n",
       "4       1      1        88             0   \n",
       "\n",
       "                                        text_process  \n",
       "0  [deed, reason, earthquake, may, allah, forgive...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [resident, asked, 'shelter, place, notified, o...  \n",
       "3  [13,000, people, receive, wildfire, evacuation...  \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdb94ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deed', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'u']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.text_process[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1c50d",
   "metadata": {},
   "source": [
    "## ML : Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1738d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df2['text_process'], df2['target'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e8440",
   "metadata": {},
   "source": [
    "## ML : Word Embedding - SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec9b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer()\n",
    "#X_train_vec = vectorizer.fit_transform(X_train)\n",
    "#X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0f50eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23598405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4996    [stfxuniversity, people, died, human, experime...\n",
       "3263    [engulfed, low, self-image, take, quiz, http, ...\n",
       "4907    [cia, hey, guy, 's, stopped, massacre, send, c...\n",
       "2855    [elem, pomo, helping, displaced, rocky, fire, ...\n",
       "4716    [morning_joe, reince, presssec, joe, ur, smart...\n",
       "                              ...                        \n",
       "5226    [auntiedote, rioslade, locke_wiggins, akarb74,...\n",
       "5390    [dream, magic, linden, method, lite, version, ...\n",
       "860     [omron, hem-712c, automatic, blood, pressure, ...\n",
       "7603    [father-of-three, lost, control, car, overtaki...\n",
       "7270    [pawsox, owner, public, return, whirlwind, tri...\n",
       "Name: text_process, Length: 6090, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c0a237d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 1851)\\t0.20827049400579561\\n  (0, 19774)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 3843)\\t0.385143605810349\\n  (0, 16611)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 7014)\\t0.21673027088707333\\n  (0, 13936)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 3797)\\t0.25856984887413254\\n  (0, 20607)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 16700)\\t0.23312651658438782\\n  (0, 9947)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0    (0, 1851)\\t0.20827049400579561\\n  (0, 19774)...\n",
       "1    (0, 3843)\\t0.385143605810349\\n  (0, 16611)\\t...\n",
       "2    (0, 7014)\\t0.21673027088707333\\n  (0, 13936)...\n",
       "3    (0, 3797)\\t0.25856984887413254\\n  (0, 20607)...\n",
       "4    (0, 16700)\\t0.23312651658438782\\n  (0, 9947)..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec = tfidf.fit_transform(df2['text'].to_numpy())\n",
    "pd.DataFrame(X_train_vec).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ce51f94",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31440\\2351602616.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test_vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2099\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"The TF-IDF vectorizer is not fitted\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2101\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2102\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \"\"\"\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "X_test_vec = tfidf.transform(X_test)\n",
    "X_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b647679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "#feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd13c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in X_train_vec.nonzero()[1]:\n",
    " #   print (feature_names[col], ' - ', X_train_vec[0, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86447b",
   "metadata": {},
   "source": [
    "## ML : SVM avec sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciation\n",
    "model_SVC = SVC() #kernel = 'linear', gamma = 'scale', shrinking = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad76353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "model_SVC.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6eda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul de la précision\n",
    "model_SVC.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prédiction\n",
    "pred = pd.Series([\"Help me, my house is on fire and all the forest is burning\"])\n",
    "X_test_vec = tfidf.transform(pred)\n",
    "prediction = model_SVC.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage des résultats\n",
    "resultat = \"Résultat : \"\n",
    "if prediction[0] == 0:\n",
    "    resultat = resultat + \"NO DISASTER\"\n",
    "if prediction[0] == 1:\n",
    "    resultat = resultat + \"DISASTER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prédiction\n",
    "pred = pd.Series([\"Hello\"])\n",
    "X_test_vec = tfidf.transform(pred)\n",
    "prediction = model_SVC.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage des résultats\n",
    "resultat = \"Résultat : \"\n",
    "if prediction[0] == 0:\n",
    "    resultat = resultat + \"NO DISASTER\"\n",
    "if prediction[0] == 1:\n",
    "    resultat = resultat + \"DISASTER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48308c",
   "metadata": {},
   "source": [
    "## ML : xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f315c",
   "metadata": {},
   "source": [
    "The data is stored in a DMatrix object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26df0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_vec).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d557785",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbce5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "dtrain = xgb.DMatrix(X_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "dtrain = xgb.DMatrix(X_train_vec, y_train)#, enable_categorical=True)\n",
    "dtest = xgb.DMatrix(X_test_vec, y_test) #, enable_categorical=True)\n",
    "np.asarray(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "param = {'objective': 'binary:logistic', 'eval_metric': 'auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96de787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "model_xgb = xgb.train(param, dtrain, num_boost_round=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model_xgb.predict(dtest)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd695a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prédiction\n",
    "pred = pd.Series([\"Hello\"])\n",
    "X_test_vec = tfidf.transform(pred)\n",
    "dpred = xgb.DMatrix(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_xgb.predict(dpred)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = sum(prediction == y_test) / len(y_test)\n",
    "#print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7ff1b",
   "metadata": {},
   "source": [
    "### API SKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5577f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_skl = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4c389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19835a1d",
   "metadata": {},
   "source": [
    "## ML : SKL Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac36bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec_arr = np.asarray(X_train_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff102bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec_arr_df = pd.DataFrame(X_train_vec_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec_arr_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16151dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_logreg = sm.Logit(y_train, X_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_logreg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model accuracy (%) : {:.1f}'.format(logreg.score(X_test_vec, y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_test, logreg.predict(X_test_vec))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test_vec)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
