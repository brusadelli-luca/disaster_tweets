{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9167ac13",
   "metadata": {},
   "source": [
    "# Eye of the Emergency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd82f9",
   "metadata": {},
   "source": [
    "## Libraries and dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9486461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\utile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\utile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\utile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\utile\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1db272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_tweets.csv')\n",
    "test_df = pd.read_csv('test_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae1b3b",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2db9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = pd.DataFrame(pd.value_counts(train_df.text))\n",
    "unique_count = unique_count.reset_index()\n",
    "unique_count.columns = ['text', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1460ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_unique = pd.DataFrame(train_df.text.unique(), columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58be70b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df_unique = train_df_unique.merge(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1755287",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_count = train_df.merge(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16e780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = pd.DataFrame([len(txt) for txt in train_df.text], columns = ['len_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee09db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_len = pd.concat([train_df_count, lg], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea3fac",
   "metadata": {},
   "source": [
    "### Location Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2127602",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_location = pd.DataFrame([int(type(ct)==str) for ct in train_df_len.location], columns=['location_enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ac0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_location = pd.concat([train_df_len, encode_location], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c7ed3",
   "metadata": {},
   "source": [
    "####  Construisez votre propre pipeline de traitement de texte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf51adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df_location.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1cd72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = train_df_location.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b01dc",
   "metadata": {},
   "source": [
    "##### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3273694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text_tokens'] = df['text'].apply(lambda x: word_tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e16392f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72bdeac",
   "metadata": {},
   "source": [
    "#### Suppression des stops words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d86b7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c042ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text_tokens'] = df['text_tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04c08d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbb7e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text_tokens'] = df['text_tokens'].apply(lambda x: [word for word in x if word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "835bea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "411aeb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text_tokens'] = df['text_tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a63ecdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[list for list in df.text_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46a45e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "182cd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "\n",
    "\n",
    "def text_processing(text):\n",
    "     #Charger les stop-words en anglais\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Initialiser le lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Appliquer la tokenisation à tous les textes\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Supprimer les ponctuations\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "\n",
    "    # Supprimer les stop-words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "\n",
    "    # Appliquer la lemmatisation à tous les tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "# Charger l'ensemble de données\n",
    "#df = pd.read_csv('votre_fichier.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "009c97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction Cleanup à tous les textes dans la colonne \"text\"\n",
    "df2['text_process'] = df['text'].apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25288ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>count</th>\n",
       "      <th>len_text</th>\n",
       "      <th>location_enc</th>\n",
       "      <th>text_process</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>resident asked 'shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>13,000 people receive wildfire evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  count  len_text  location_enc  \\\n",
       "0       1      1        69             0   \n",
       "1       1      1        38             0   \n",
       "2       1      1       133             0   \n",
       "3       1      1        65             0   \n",
       "4       1      1        88             0   \n",
       "\n",
       "                                        text_process  \n",
       "0         deed reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident asked 'shelter place notified officer...  \n",
       "3  13,000 people receive wildfire evacuation orde...  \n",
       "4  got sent photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32698996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deed reason earthquake may allah forgive u'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.text_process[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef2775aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df2['text_process'], df2['target'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c31a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "637af129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6090x17514 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 65731 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
